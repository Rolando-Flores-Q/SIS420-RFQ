{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir estados, acciones y recompensas\n",
        "estados = ['A', 'B', 'C', 'D', 'E']\n",
        "acciones = ['izquierda', 'derecha']\n",
        "recompensas = np.array([\n",
        "    [0, -1],\n",
        "    [1, 0],\n",
        "    [-1, 1],\n",
        "    [1, 0],\n",
        "    [-1, 0]\n",
        "])\n",
        "\n",
        "# Inicializar la matriz Q con ceros\n",
        "Q = np.zeros((len(estados), len(acciones)))\n",
        "\n",
        "# Parámetros del algoritmo\n",
        "gamma = 0.9  # Tasa de descuento\n",
        "alpha = 0.5  # Tasa de aprendizaje (reducida para convergencia más lenta pero más estable)\n",
        "epsilon = 0.9  # Probabilidad de exploración\n",
        "\n",
        "# Ejecutar el algoritmo Q-learning\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    # Inicializar el estado de manera aleatoria\n",
        "    estado = np.random.randint(0, len(estados))\n",
        "\n",
        "    while True:\n",
        "        # Selección de acción: exploración o explotación\n",
        "        if np.random.rand() < epsilon:\n",
        "            accion = np.random.randint(0, len(acciones))\n",
        "        else:\n",
        "            accion = np.argmax(Q[estado])\n",
        "\n",
        "        # Obtener la recompensa y el próximo estado según la acción tomada\n",
        "        recompensa = recompensas[estado, accion]\n",
        "        proximo_estado = (estado + 1) % len(estados)\n",
        "\n",
        "        # Actualizar la matriz Q con la fórmula de Q-learning\n",
        "        Q[estado, accion] = Q[estado, accion] + alpha * (recompensa + gamma * np.max(Q[proximo_estado, :]) - Q[estado, accion])\n",
        "\n",
        "        # Actualizar el estado al próximo estado\n",
        "        estado = proximo_estado\n",
        "\n",
        "        # Si se alcanza el estado final (A), terminar el episodio\n",
        "        if estado == 0:\n",
        "            break\n",
        "\n",
        "# Imprimir la matriz Q resultante\n",
        "print(\"Matriz Q:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8_SeNaDE6je",
        "outputId": "8078bfd6-1a4c-48e3-cda8-47c72cb51a43"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz Q:\n",
            "[[5.95583508 4.95582211]\n",
            " [6.6176019  5.61758817]\n",
            " [4.24178174 6.24178935]\n",
            " [5.82421986 4.82421555]\n",
            " [4.36024782 5.36025092]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aqui se podra observar las probabilidades y la accion que se toma segun a la probabilidad mas alta"
      ],
      "metadata": {
        "id": "UpzVkWA5M6A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir la matriz Q resultante del algoritmo Q-learning\n",
        "print(\"Matriz Q:\")\n",
        "print(Q)\n",
        "# Calcular las probabilidades de cada acción en cada estado utilizando la función softmax\n",
        "probs = np.exp(Q) / np.sum(np.exp(Q), axis=1, keepdims=True)\n",
        "# Obtener el índice de la acción con mayor y menor probabilidad en cada fila\n",
        "acciones_maximas = np.argmax(probs, axis=1)\n",
        "acciones_minimas = np.argmin(probs, axis=1)\n",
        "# Redondear las probabilidades a un decimal y mostrarlas como porcentajes\n",
        "probabilidades_acciones = np.round(probs * 100, 1)\n",
        "# Iterar a través de cada estado y mostrar las probabilidades y la acción con mayor probabilidad\n",
        "for estado, acciones_probabilidades, accion_maxima in zip(estados, probabilidades_acciones, acciones_maximas):\n",
        "    print(f\"\\n Estado {estado}:\")\n",
        "    # Iterar a través de cada acción en el estado y mostrar la probabilidad\n",
        "    for accion, probabilidad in zip(acciones, acciones_probabilidades):\n",
        "        print(f\"  Probabilidad para la acción {accion}: {probabilidad:.1f}%\")\n",
        "    # Imprimir líneas separadoras\n",
        "    print(\"#\" * 25)\n",
        "    # Mostrar la acción con la probabilidad máxima como el camino a tomar en ese estado\n",
        "    print(f\"  Camino a tomar: {acciones[accion_maxima]}\")\n",
        "    # Imprimir líneas separadoras\n",
        "    print(\"#\" * 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjUxqtpAM5Wd",
        "outputId": "cb26765c-0750-4eb3-85ba-60411c5cbe48"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz Q:\n",
            "[[5.95583508 4.95582211]\n",
            " [6.6176019  5.61758817]\n",
            " [4.24178174 6.24178935]\n",
            " [5.82421986 4.82421555]\n",
            " [4.36024782 5.36025092]]\n",
            "\n",
            " Estado A:\n",
            "  Probabilidad para la acción izquierda: 73.1%\n",
            "  Probabilidad para la acción derecha: 26.9%\n",
            "#########################\n",
            "  Camino a tomar: izquierda\n",
            "#########################\n",
            "\n",
            " Estado B:\n",
            "  Probabilidad para la acción izquierda: 73.1%\n",
            "  Probabilidad para la acción derecha: 26.9%\n",
            "#########################\n",
            "  Camino a tomar: izquierda\n",
            "#########################\n",
            "\n",
            " Estado C:\n",
            "  Probabilidad para la acción izquierda: 11.9%\n",
            "  Probabilidad para la acción derecha: 88.1%\n",
            "#########################\n",
            "  Camino a tomar: derecha\n",
            "#########################\n",
            "\n",
            " Estado D:\n",
            "  Probabilidad para la acción izquierda: 73.1%\n",
            "  Probabilidad para la acción derecha: 26.9%\n",
            "#########################\n",
            "  Camino a tomar: izquierda\n",
            "#########################\n",
            "\n",
            " Estado E:\n",
            "  Probabilidad para la acción izquierda: 26.9%\n",
            "  Probabilidad para la acción derecha: 73.1%\n",
            "#########################\n",
            "  Camino a tomar: derecha\n",
            "#########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Una vez entrenado el modelo aqui podemos utilizar colocando el estado que desamos y lo muestra correctamente porque el modelo ya esta entrenado**"
      ],
      "metadata": {
        "id": "2BJ2RxOiMavf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir la matriz Q resultante del algoritmo Q-learning\n",
        "print(\"\\nMatriz Q:\")\n",
        "print(Q)\n",
        "\n",
        "# Utilizar la política aprendida para tomar decisiones\n",
        "def tomar_decision(estado):\n",
        "    return np.argmax(Q[estado])\n",
        "\n",
        "# Ejemplo de uso\n",
        "estado_inicial = 3  # Puedes cambiar esto con el estado desde el que quieres empezar\n",
        "print(f\"\\nTomar decisiones desde el estado {estados[estado_inicial]}:\")\n",
        "\n",
        "# Tomar decisiones durante 5 pasos\n",
        "for _ in range(5):\n",
        "    accion_elegida = tomar_decision(estado_inicial) # Obtener la acción elegida utilizando la política aprendida\n",
        "    print(f\"Desde {estados[estado_inicial]}, tomar acción: {acciones[accion_elegida]}\")  # Imprimir la acción elegida desde el estado actual\n",
        "    # Actualizar el estado al próximo estado\n",
        "    estado_inicial = (estado_inicial + 1) % len(estados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBptLTVyJbw6",
        "outputId": "841565bd-30a2-4fde-e909-643116050058"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matriz Q:\n",
            "[[5.95583508 4.95582211]\n",
            " [6.6176019  5.61758817]\n",
            " [4.24178174 6.24178935]\n",
            " [5.82421986 4.82421555]\n",
            " [4.36024782 5.36025092]]\n",
            "\n",
            "Tomar decisiones desde el estado D:\n",
            "Desde D, tomar acción: izquierda\n",
            "Desde E, tomar acción: derecha\n",
            "Desde A, tomar acción: izquierda\n",
            "Desde B, tomar acción: izquierda\n",
            "Desde C, tomar acción: derecha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdn3zReqJbzL"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCkc_RCKJb7j"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vznDe0LiJcBk"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "XfGWY6rhS0-q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir los estados y acciones\n",
        "estados = ['A', 'B', 'C', 'D','E']\n",
        "acciones = ['column 1', 'column 2', 'column 3', 'column 4']   #'column 1', 'column 2', 'column 3', 'column 4'\n",
        "\n",
        "# Definir la matriz de recompensas\n",
        "recompensas = np.array([\n",
        "    [1, 1, 0, 0],\n",
        "    [1, -1, 0, -1],\n",
        "    [0, 1, -1, 0],\n",
        "    [0, 1, 1, -1],\n",
        "    [-1, 0, 1, 0]\n",
        "])\n",
        "\n",
        "# Inicializar la matriz Q con ceros\n",
        "Q = np.zeros((len(estados), len(acciones)))\n",
        "\n",
        "# Hiperparámetros\n",
        "gamma = 0.9  # Tasa de descuento\n",
        "alpha = 0.5  # Tasa de aprendizaje\n",
        "epsilon = 0.9  # Probabilidad de exploración\n",
        "\n",
        "# Ejecutar el algoritmo Q-learning\n",
        "num_episodes = 100\n",
        "for episode in range(num_episodes):\n",
        "    estado = np.random.randint(0, len(estados))  # Estado inicial aleatorio\n",
        "\n",
        "    while True:\n",
        "        # Tomar una acción: exploración o explotación\n",
        "        if np.random.rand() < epsilon:\n",
        "            accion = np.random.randint(0, len(acciones))  # Acción aleatoria\n",
        "        else:\n",
        "            accion = np.argmax(Q[estado])\n",
        "\n",
        "        # Obtener la recompensa y el próximo estado\n",
        "        recompensa = recompensas[estado, accion]\n",
        "        proximo_estado = (estado + 1) % len(estados)  # Transición determinística\n",
        "\n",
        "        # Actualizar la matriz Q con la fórmula de Q-learning\n",
        "        Q[estado, accion] = Q[estado, accion] + alpha * (recompensa + gamma * np.max(Q[proximo_estado, :]) - Q[estado, accion])\n",
        "\n",
        "        # Actualizar el estado actual\n",
        "        estado = proximo_estado\n",
        "\n",
        "        if estado == 0:  # Si se alcanza el estado final (A), terminar el episodio\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir la matriz Q resultante\n",
        "print(\"Tabla Q:\")\n",
        "print(Q)\n",
        "\n",
        "# Aplicar la función softmax para obtener las probabilidades\n",
        "probs = np.exp(Q) / np.sum(np.exp(Q), axis=1, keepdims=True)\n",
        "\n",
        "# Obtener el índice de la acción con mayor probabilidad en cada fila\n",
        "acciones_maximas = np.argmax(probs, axis=1)\n",
        "acciones_minimas = np.argmin(probs, axis=1)\n",
        "\n",
        "# Obtener los valores de probabilidad para cada acción en cada fila\n",
        "probabilidades_acciones = np.round(probs * 100, 1)\n",
        "\n",
        "# Definir los estados y acciones correspondientes\n",
        "estados = ['A', 'B', 'C', 'D', 'E']\n",
        "acciones = ['column 1', 'column 2', 'column 3', 'column 4']\n",
        "\n",
        "# Imprimir los resultados\n",
        "for estado, acciones_probabilidades, acciones_maximas in zip(estados, probabilidades_acciones, acciones_maximas):\n",
        "    print(f\"\\n Estado {estado}:\")\n",
        "    for accion, probabilidad in zip(acciones, acciones_probabilidades):\n",
        "        print(f\"  Probabilidad para la acción {accion}: {probabilidad:.1f}%\")\n",
        "    print(\"#\" * 25)\n",
        "    print(f\"  Camino a tomar: {acciones[acciones_maximas]}\")\n",
        "    print(\"#\" * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igtc_M7yhEDL",
        "outputId": "057f84f9-86bb-49b6-e3b8-3d08372f95f0"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q:\n",
            "[[5.54082769 4.18318599 4.03202617 5.05801242]\n",
            " [6.25179099 4.08216331 5.09644718 3.61872469]\n",
            " [4.47736072 6.14525176 3.84434617 5.1004    ]\n",
            " [4.94507566 5.54759609 6.22220191 3.64638332]\n",
            " [3.18287402 3.95123471 5.96179506 4.63027959]]\n",
            "\n",
            " Estado A:\n",
            "  Probabilidad para la acción column 1: 47.7%\n",
            "  Probabilidad para la acción column 2: 12.3%\n",
            "  Probabilidad para la acción column 3: 10.6%\n",
            "  Probabilidad para la acción column 4: 29.4%\n",
            "#########################\n",
            "  Camino a tomar: column 1\n",
            "#########################\n",
            "\n",
            " Estado B:\n",
            "  Probabilidad para la acción column 1: 66.6%\n",
            "  Probabilidad para la acción column 2: 7.6%\n",
            "  Probabilidad para la acción column 3: 21.0%\n",
            "  Probabilidad para la acción column 4: 4.8%\n",
            "#########################\n",
            "  Camino a tomar: column 1\n",
            "#########################\n",
            "\n",
            " Estado C:\n",
            "  Probabilidad para la acción column 1: 11.5%\n",
            "  Probabilidad para la acción column 2: 61.0%\n",
            "  Probabilidad para la acción column 3: 6.1%\n",
            "  Probabilidad para la acción column 4: 21.4%\n",
            "#########################\n",
            "  Camino a tomar: column 2\n",
            "#########################\n",
            "\n",
            " Estado D:\n",
            "  Probabilidad para la acción column 1: 15.0%\n",
            "  Probabilidad para la acción column 2: 27.3%\n",
            "  Probabilidad para la acción column 3: 53.6%\n",
            "  Probabilidad para la acción column 4: 4.1%\n",
            "#########################\n",
            "  Camino a tomar: column 3\n",
            "#########################\n",
            "\n",
            " Estado E:\n",
            "  Probabilidad para la acción column 1: 4.3%\n",
            "  Probabilidad para la acción column 2: 9.2%\n",
            "  Probabilidad para la acción column 3: 68.5%\n",
            "  Probabilidad para la acción column 4: 18.1%\n",
            "#########################\n",
            "  Camino a tomar: column 3\n",
            "#########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Tabla Q:\")\n",
        "print(Q)\n",
        "\n",
        "# Utilizar la política aprendida para tomar decisiones\n",
        "def tomar_decision(estado):\n",
        "    return np.argmax(Q[estado])\n",
        "\n",
        "# Ejemplo de uso: Tomar decisiones desde el estado inicial\n",
        "estado_inicial = 3  # Puedes cambiar esto con el estado desde el que quieres empezar\n",
        "print(f\"\\nTomar decisiones desde el estado {estados[estado_inicial]}:\")\n",
        "for _ in range(5):  # Tomar decisiones durante 5 pasos\n",
        "    accion_elegida = tomar_decision(estado_inicial)\n",
        "    print(f\"Desde {estados[estado_inicial]}, tomar acción: {acciones[accion_elegida]}\")\n",
        "    estado_inicial = (estado_inicial + 1) % len(estados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeZbaEKuOtcb",
        "outputId": "8e62313a-08b8-4238-b215-142b893eafea"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q:\n",
            "[[5.54082769 4.18318599 4.03202617 5.05801242]\n",
            " [6.25179099 4.08216331 5.09644718 3.61872469]\n",
            " [4.47736072 6.14525176 3.84434617 5.1004    ]\n",
            " [4.94507566 5.54759609 6.22220191 3.64638332]\n",
            " [3.18287402 3.95123471 5.96179506 4.63027959]]\n",
            "\n",
            "Tomar decisiones desde el estado D:\n",
            "Desde D, tomar acción: column 3\n",
            "Desde E, tomar acción: column 3\n",
            "Desde A, tomar acción: column 1\n",
            "Desde B, tomar acción: column 1\n",
            "Desde C, tomar acción: column 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}